[
  {"id": 1, "use": true, "param": "hidden_layer_sizes", "type": "array", "default_value": [100], "name": "Размер скрытого слоя", "info": "Представляет количество нейронов в i-м скрытом слое.", "diap": [1, 5, 1, 100]},
  {"id": 2, "use": true, "param": "activation", "type": "string", "default_value": {"label": "relu", "value": "relu"}, "name": "Функция активации", "info": "Функция активации скрытого слоя. «идентичность», активация без операций, полезная для реализации линейного узкого места, возвращает f(x) = x «логистика», логистическая сигмовидная функция, возвращает f(x) = 1/(1 + exp(-x)). «tanh», гиперболическая функция tan, возвращает f(x) = tanh(x). «relu», выпрямленная линейная единичная функция, возвращает f(x) = max(0, x)", "diap": ["identity", "logistic", "tanh", "relu"]},
  {"id": 3, "use": true, "param": "solver", "type": "string", "default_value": {"label": "adam", "value": "adam"}, "name": "Решатель для оптимизации веса.", "info": "Решатель для оптимизации веса. «lbfgs» — это оптимизатор семейства квазиньютоновских методов. «sgd» относится к стохастическому градиентному спуску. «Адам» относится к оптимизатору на основе стохастического градиента, предложенному Кингмой, Дидериком и Джимми Ба. Примечание. Решатель по умолчанию «Адам» довольно хорошо работает с относительно большими наборами данных (с тысячами обучающих выборок и более) с точки зрения как времени обучения, так и оценки проверки. Однако для небольших наборов данных «lbfgs» может сходиться быстрее и работать лучше.", "diap": ["lbfgs", "sgd", "adam"]},
  {"id": 4, "use": true, "param": "alpha", "type": "float", "default_value": 0.0001, "name": "Сила члена регуляризации L2.", "info": "Сила члена регуляризации L2. Член регуляризации L2 делится на размер выборки и добавляется к потерям.", "diap": [0]},
  {"id": 5, "use": false, "param": "batch_size", "type": "int", "default_value": 1, "name": "Размер мини пакетов обучения", "info": "Размер мини-пакетов для стохастических оптимизаторов. Если решателем является «lbfgs», классификатор не будет использовать мини-пакет. Если установлено значение «авто», Batch_size=min(200, n_samples).", "diap": [0]},
  {"id": 6, "use": true, "param": "learning_rate", "type": "string", "default_value": {"label": "constant", "value": "constant"}, "name": "Рейтинг обучения", "info": "График обучения для обновлений веса.\n\n«константа» — это постоянная скорость обучения, заданная параметром «learning_rate_init».\n\n«invscaling» постепенно снижает скорость обучения на каждом временном шаге «t», используя показатель обратного масштабирования «power_t». effect_learning_rate = Learning_rate_init / pow(t, power_t)\n\n«Адаптивный» поддерживает постоянную скорость обучения на уровне «learning_rate_init», пока потери на обучение продолжают уменьшаться. Каждый раз, когда две последовательные эпохи не могут уменьшить потери обучения хотя бы на tol или не могут увеличить оценку валидации хотя бы на tol, если включен параметр «early_stopping», текущая скорость обучения делится на 5.\n\nИспользуется только тогда, когда Решатель для оптимизации веса.='sgd'.", "diap": ["constant", "invscaling", "adaptive"]},
  {"id": 7, "use": true, "param": "learning_rate_init", "type": "float", "default_value": 0.001, "name": "Рейтинг обучения числовой", "info": "Используемая начальная скорость обучения. Он контролирует размер шага при обновлении весов. Используется только в том случае, если Решатель для оптимизации веса.=’sgd’ или ‘adam’.", "diap": [0]},
  {"id": 8, "use": true, "param": "power_t", "type": "float", "default_value": 0.5, "name": "Показатель степени обратного масштабирования скорости обучения.", "info": "Показатель степени обратного масштабирования скорости обучения. Он используется для обновления эффективной скорости обучения, когда для параметра Learning_rate установлено значение «invscaling». Используется только тогда, когда Решатель для оптимизации веса=’sgd’.", "diap": [0]},
  {"id": 9, "use": true, "param": "max_iter", "type": "int", "default_value": 200, "name": "Максимальное кол-во иттераций обучения", "info": "Максимальное количество итераций, необходимое для сходимости решателей", "diap": [1]},
  {"id": 10, "use": true, "param": "shuffle", "type": "bool", "default_value": true, "name": "Замешивание", "info": "Перетасовывать ли образцы на каждой итерации. Используется только в том случае, если решатель=’sgd’ или ‘adam’.", "diap": []},
  {"id": 11, "use": true, "param": "random_state", "type": "int", "default_value": null, "name": "Управляет случайностью оценки.", "info": "Объекты всегда случайным образом переставляются при каждом разбиении, даже если splitterустановлено значение \"best\". Когда , алгоритм будет выбирать случайным образом для каждого разделения, прежде чем найти среди них лучшее разделение. Но наилучшее найденное разделение может различаться в разных прогонах, даже если . Это тот случай, когда улучшение критерия одинаково для нескольких разбиений и одно разбиение должно быть выбрано случайным образом. Чтобы получить детерминированное поведение во время подгонки, необходимо зафиксировать целое число. .max_features < n_featuresmax_featuresmax_features=n_featuresrandom_state", "diap": [0]},
  {"id": 12, "use": true, "param": "tol", "type": "float", "default_value": 0.0001, "name": "Критерий остановки.", "info": "Допуск по критериям остановки.", "diap": [0]},
  {"id": 13, "use": true, "param": "warm_start", "type": "bool", "default_value": false, "name": "Тепловое начало", "info": "Если установлено значение True, повторно используйте решение предыдущего вызова в качестве инициализации, в противном случае просто сотрите предыдущее решение. Бесполезно для линейного решателя", "diap": null},
  {"id": 14, "use": true, "param": "momentum", "type": "float", "default_value": 0.9, "name": "Импульс для обновления градиентного спуска.", "info": "Импульс для обновления градиентного спуска. Должно быть от 0 до 1. Используется только в том случае, если решатель=’sgd’.", "diap": [0, 1]},
  {"id": 15, "use": true, "param": "nesterovs_momentum", "type": "bool", "default_value": true, "name": "Нестеровский импульс", "info": "Стоит ли использовать импульс Нестерова. Используется только тогда, когда решатель=’sgd’ и импульс > 0.", "diap": []},
  {"id": 16, "use": true, "param": "early_stopping", "type": "bool", "default_value": false, "name": "Ранняя остановка", "info": "Следует ли использовать раннюю остановку для прекращения обучения, если показатель проверки не улучшается. Если установлено значение true, он автоматически отложит 10 % данных обучения для проверки и прекратит обучение, когда оценка проверки не улучшится как минимум на Допуск по критериям остановки в течение Количество итераций без изменений последовательных эпох. Разделение является стратифицированным, за исключением настройки с несколькими метками. Если ранняя остановка имеет значение False, то обучение прекращается, когда потери при обучении не улучшаются более чем на Допуск по критериям остановки за Количество итераций без изменений последовательных проходов по обучающему набору. Эффективно только тогда, когда решатель=’sgd’ или ‘adam’.", "diap": []},
  {"id": 17, "use": true, "param": "validation_fraction", "type": "float", "default_value": 0.1, "name": "Валидационное разбиение", "info": "Доля обучающих данных, которые необходимо отложить в качестве набора проверки для ранней остановки. Значения должны находиться в диапазоне (0,0, 1,0). Используется только в том случае, если для параметра n_iter_no_change установлено целое число.", "diap": [0, 1]},
  {"id": 18, "use": true, "param": "is_permutate", "type": "bool", "default_value": false, "name": "Считать значимость факторов", "info": "Следует ли высчитывать значимость факторов, учтите что время их выполнения много больше обучения модели и предпочтительно использовать это уже на обученных моделях для улучшения результатов", "diap": null}
]